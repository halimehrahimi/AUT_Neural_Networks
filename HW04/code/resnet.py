# -*- coding: utf-8 -*-
"""resnet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DttfchEkBSf-f-lwMfnTuLiP0AGLo6qL
"""

import numpy as np
from skimage import io
import glob
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
import matplotlib.pyplot as plt
#from sklearn.metrics import confusion_matrix
#from sklearn.metrics import accuracy_score

from skimage import io,transform
import glob
imlist=[]
ylabel=[]
for filename in glob.glob("/content/drive/MyDrive/ANN/Datasets/IndoorOutdoorimageclassification/indoor/indoor/*.jpg"):
    im = io.imread(filename)
    img=tf.cast(im, tf.float32)
    resized_images=tf.image.resize(img, (128, 128))
    imlist.append(resized_images)
    ylabel.append(0)
for filename in glob.glob("/content/drive/MyDrive/ANN/Datasets/IndoorOutdoorimageclassification/outdoor/outdoor/*.jpg"):
    im = io.imread(filename)
    img=tf.cast(im, tf.float32)
    resized_images=tf.image.resize(img, (128, 128))
    imlist.append(resized_images)
    ylabel.append(1)

imlist=np.array(imlist)
imlist /= 255
ylabel=np.array(ylabel)
print(imlist.shape)
print(ylabel.shape)

np.random.seed(seed=0)
train_ind = np.random.randint(low = 0, high = ylabel.size, size = int(0.7*ylabel.size))
train_data = imlist[train_ind]
train_label = ylabel[train_ind]
imlist = np.delete(imlist, train_ind, axis=0)
ylabel = np.delete(ylabel, train_ind, axis=0)
test_ind = np.random.randint(low = 0, high = ylabel.size, size = int((1/3)*ylabel.size))
test_data = imlist[test_ind]
test_label = ylabel[test_ind]
val_data = np.delete(imlist, test_ind, axis=0)
val_label = np.delete(ylabel, test_ind)
print(train_data.shape,train_label.shape)
print(val_data.shape, val_label.shape)
print(test_data.shape, test_label.shape)
print(train_data[0].shape)
print(val_data[0].shape)
print(test_data[0].shape)

tf.keras.backend.clear_session()

class EarlyStoppingCallback(tf.keras.callbacks.Callback):
  def __init__(self, patience=0):
    super(EarlyStoppingCallback, self).__init__()
    self.patience = patience

  def on_train_begin(self, logs=None):
    self.best = -np.Inf
    self.wait = 0
    self.stopped_epoch = 0

  def on_epoch_end(self, epoch, logs=None):
    current_accuracy = logs.get("val_accuracy")
    if np.greater(current_accuracy, self.best):
      self.best = current_accuracy
      self.wait = 0
      self.best_weights = self.model.get_weights()
    else:
      self.wait += 1
      if self.wait >= self.patience:
        self.stopped_epoch = epoch
        self.model.stop_training = True
        self.model.set_weights(self.best_weights)

  def on_train_end(self, logs=None):
    if self.stopped_epoch > 0:
      print("epoch: %d: early stopping" % self.stopped_epoch)

base_model = tf.keras.applications.ResNet101(weights = 'imagenet', include_top = False, input_shape = (128,128,3))

base_model.trainable = False

cls_callback = EarlyStoppingCallback(patience=5)

x = tf.keras.layers.Flatten()(base_model.output)
x = tf.keras.layers.Dense(512, activation='relu')(x)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(256, activation='relu')(x)
x = tf.keras.layers.Dropout(0.2)(x)
predictions = tf.keras.layers.Dense(2, activation = 'softmax')(x)

head_model = tf.keras.Model(inputs = base_model.input, outputs = predictions)
head_model.compile(optimizer='adam', loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])

history = head_model.fit(train_data, train_label, batch_size=64, epochs=40, validation_data=(val_data, val_label), callbacks=cls_callback)

fig, axs = plt.subplots(2, 1, figsize=(15,15))
axs[0].plot(history.history['loss'])
axs[0].plot(history.history['val_loss'])
axs[0].title.set_text('Training Loss vs Validation Loss')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Loss')
axs[0].legend(['Train','Val'])
axs[1].plot(history.history['accuracy'])
axs[1].plot(history.history['val_accuracy'])
axs[1].title.set_text('Training Accuracy vs Validation Accuracy')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Accuracy')
axs[1].legend(['Train', 'Val'])

head_model.evaluate(test_data, test_label)

test_pred = np.argmax(head_model.predict(test_data), axis=-1)
confmat = tf.math.confusion_matrix(test_label, test_pred)
print(confmat)

for layer in base_model.layers[-7:]:
  layer.trainable = True

head_model.summary()

head_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.000001), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])
history = head_model.fit(train_data, train_label, batch_size=64, epochs=40, validation_data=(val_data, val_label), callbacks=cls_callback)

fig, axs = plt.subplots(2, 1, figsize=(15,15))
axs[0].plot(history.history['loss'])
axs[0].plot(history.history['val_loss'])
axs[0].title.set_text('Training Loss vs Validation Loss')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Loss')
axs[0].legend(['Train','Val'])
axs[1].plot(history.history['accuracy'])
axs[1].plot(history.history['val_accuracy'])
axs[1].title.set_text('Training Accuracy vs Validation Accuracy')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Accuracy')
axs[1].legend(['Train', 'Val'])

head_model.evaluate(test_data, test_label)

test_pred = np.argmax(head_model.predict(test_data), axis=-1)
confmat = tf.math.confusion_matrix(test_label, test_pred)
print(confmat)